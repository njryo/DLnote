{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-rc0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_toy_dataset(num_data):\n",
    "    X = np.random.randn(num_data, 3)\n",
    "    y = 3 * X[:, 0] - 2 * X[:, 1]**3 + 2 * X[:, 2]**2 + 0.5 * np.random.randn(num_data)\n",
    "    y = y[:, np.newaxis]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Dense(10, activation=\"relu\", input_dim=3),\n",
    "        keras.layers.Dense(10, activation=\"relu\"),\n",
    "        keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = 40000\n",
    "N_test = 4000\n",
    "\n",
    "num_epochs = 10\n",
    "learning_rate = 0.01\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get toy datasets\n",
    "X_train, y_train = make_toy_dataset(N_train)\n",
    "X_test, y_test = make_toy_dataset(N_test)\n",
    "\n",
    "# make datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=512).batch(batch_size)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "test_dataset = test_dataset.batch(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "40000/40000 [==============================] - 2s 60us/sample - loss: 9.3465 - mae: 1.6021 - val_loss: 3.0168 - val_mae: 0.9789\n",
      "Epoch 2/10\n",
      "40000/40000 [==============================] - 1s 35us/sample - loss: 2.5969 - mae: 1.0369 - val_loss: 2.8398 - val_mae: 1.0157\n",
      "Epoch 3/10\n",
      "40000/40000 [==============================] - 1s 33us/sample - loss: 2.1557 - mae: 0.9323 - val_loss: 2.3046 - val_mae: 0.8977\n",
      "Epoch 4/10\n",
      "40000/40000 [==============================] - 1s 35us/sample - loss: 1.6998 - mae: 0.8437 - val_loss: 1.7466 - val_mae: 0.6958\n",
      "Epoch 5/10\n",
      "40000/40000 [==============================] - 1s 35us/sample - loss: 1.4499 - mae: 0.7754 - val_loss: 1.5472 - val_mae: 0.7006\n",
      "Epoch 6/10\n",
      "40000/40000 [==============================] - 1s 35us/sample - loss: 1.2842 - mae: 0.7407 - val_loss: 1.3635 - val_mae: 0.6287\n",
      "Epoch 7/10\n",
      "40000/40000 [==============================] - 1s 35us/sample - loss: 1.1056 - mae: 0.7160 - val_loss: 1.3598 - val_mae: 0.6406\n",
      "Epoch 8/10\n",
      "40000/40000 [==============================] - 1s 33us/sample - loss: 1.0462 - mae: 0.6992 - val_loss: 1.2696 - val_mae: 0.6071\n",
      "Epoch 9/10\n",
      "40000/40000 [==============================] - 1s 32us/sample - loss: 0.9039 - mae: 0.6674 - val_loss: 1.2978 - val_mae: 0.6696\n",
      "Epoch 10/10\n",
      "40000/40000 [==============================] - 1s 32us/sample - loss: 0.8644 - mae: 0.6467 - val_loss: 1.5995 - val_mae: 0.8621\n",
      "14.67579698562622 (sec)\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\n",
    "model.compile(optimizer, loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "start = time.time()\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=num_epochs, validation_data=(X_test, y_test))\n",
    "end = time.time()\n",
    "\n",
    "print(end - start, \"(sec)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0827 19:49:31.786652 11664 base_layer.py:1772] Layer dense_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: loss=7.460, val_loss=2.745130, mae=1.415, val_mae=0.968\n",
      "epoch 2: loss=2.092, val_loss=1.492179, mae=0.850, val_mae=0.716\n",
      "epoch 3: loss=1.300, val_loss=1.134630, mae=0.714, val_mae=0.641\n",
      "epoch 4: loss=1.130, val_loss=1.307690, mae=0.655, val_mae=0.718\n",
      "epoch 5: loss=0.907, val_loss=0.884154, mae=0.632, val_mae=0.583\n",
      "epoch 6: loss=1.239, val_loss=0.946404, mae=0.654, val_mae=0.591\n",
      "epoch 7: loss=0.888, val_loss=1.181401, mae=0.617, val_mae=0.639\n",
      "epoch 8: loss=0.714, val_loss=0.867503, mae=0.589, val_mae=0.576\n",
      "epoch 9: loss=0.718, val_loss=0.833562, mae=0.579, val_mae=0.552\n",
      "epoch 10: loss=0.765, val_loss=0.917903, mae=0.573, val_mae=0.558\n",
      "72.75276517868042 (sec)\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "loss_fn = keras.losses.MeanSquaredError()\n",
    "\n",
    "train_loss = keras.metrics.Mean()\n",
    "test_loss = keras.metrics.Mean()\n",
    "train_mae_metric = keras.metrics.MeanAbsoluteError()\n",
    "test_mae_metric = keras.metrics.MeanAbsoluteError()\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    \n",
    "    # train step \n",
    "    for x_batch, y_batch in train_dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(x_batch)\n",
    "            loss = loss_fn(y_batch, y_pred)\n",
    "        grads = tape.gradient(loss, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        \n",
    "        # accumulate metrics of one batch\n",
    "        train_loss(loss)\n",
    "        train_mae_metric(y_batch, y_pred)\n",
    "    \n",
    "    \n",
    "    # get metrics of one epoch and reset their states\n",
    "    train_loss_result = train_loss.result()\n",
    "    train_loss.reset_states()\n",
    "    \n",
    "    train_mae_result = train_mae_metric.result()\n",
    "    train_mae_metric.reset_states()\n",
    "    \n",
    "    \n",
    "    # evaluation step\n",
    "    for x_batch, y_batch in test_dataset:\n",
    "        y_pred = model(x_batch)\n",
    "        loss = loss_fn(y_batch, y_pred)\n",
    "        \n",
    "        # accumulate metrics\n",
    "        test_loss(loss)\n",
    "        test_mae_metric(y_batch, y_pred)\n",
    "    \n",
    "    \n",
    "    # get metrics and reset their states\n",
    "    test_loss_result = test_loss.result()\n",
    "    test_loss.reset_states()\n",
    "    \n",
    "    test_mae_result = test_mae_metric.result()\n",
    "    test_mae_metric.reset_states()\n",
    "    \n",
    "    \n",
    "    # print train logs\n",
    "    template = \"epoch {}: loss={:.3f}, val_loss={:3f}, mae={:.3f}, val_mae={:.3f}\"\n",
    "    print(template.format(epoch, train_loss_result, test_loss_result, train_mae_result, test_mae_result))\n",
    "\n",
    "end = time.time()\n",
    "print(end - start, \"(sec)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0827 19:50:44.817705 11664 base_layer.py:1772] Layer dense_6 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: loss=8.643, val_loss=4.620719, mae=1.442, val_mae=1.135\n",
      "epoch 2: loss=2.975, val_loss=2.078732, mae=0.994, val_mae=0.850\n",
      "epoch 3: loss=2.513, val_loss=1.819898, mae=0.918, val_mae=0.799\n",
      "epoch 4: loss=1.661, val_loss=1.325308, mae=0.825, val_mae=0.719\n",
      "epoch 5: loss=1.781, val_loss=1.311718, mae=0.796, val_mae=0.678\n",
      "epoch 6: loss=1.443, val_loss=1.115543, mae=0.751, val_mae=0.639\n",
      "epoch 7: loss=1.146, val_loss=1.039012, mae=0.706, val_mae=0.628\n",
      "epoch 8: loss=0.952, val_loss=0.927890, mae=0.667, val_mae=0.598\n",
      "epoch 9: loss=1.134, val_loss=1.366605, mae=0.683, val_mae=0.708\n",
      "epoch 10: loss=0.882, val_loss=1.005251, mae=0.657, val_mae=0.578\n",
      "29.26610279083252 (sec)\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "loss_fn = keras.losses.MeanSquaredError()\n",
    "\n",
    "train_loss = keras.metrics.Mean()\n",
    "test_loss = keras.metrics.Mean()\n",
    "train_mae_metric = keras.metrics.MeanAbsoluteError()\n",
    "test_mae_metric = keras.metrics.MeanAbsoluteError()\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    #print(\"tracing:\", model, optimizer, x, y)\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y, y_pred)\n",
    "    grads = tape.gradient(loss, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    return y_pred, loss\n",
    "\n",
    "@tf.function\n",
    "def eval_step(x, y):\n",
    "    #print(\"tracing:\", model, x, y)\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y, y_pred)\n",
    "    return y_pred, loss\n",
    "    \n",
    "start = time.time()\n",
    "    \n",
    "for epoch in range(1, num_epochs+1):\n",
    "    \n",
    "    # train step \n",
    "    for x_batch, y_batch in train_dataset:\n",
    "        \n",
    "        y_pred, loss = train_step(x_batch, y_batch)\n",
    "        \n",
    "        # accumulate metrics of one batch\n",
    "        train_loss(loss)\n",
    "        train_mae_metric(y_batch, y_pred)\n",
    "    \n",
    "    \n",
    "    # get metrics of one epoch and reset their states\n",
    "    train_loss_result = train_loss.result()\n",
    "    train_loss.reset_states()\n",
    "    \n",
    "    train_mae_result = train_mae_metric.result()\n",
    "    train_mae_metric.reset_states()\n",
    "    \n",
    "    \n",
    "    # evaluation step\n",
    "    for x_batch, y_batch in test_dataset:\n",
    "        y_pred, loss = eval_step(x_batch, y_batch)\n",
    "        \n",
    "        # accumulate metrics\n",
    "        test_loss(loss)\n",
    "        test_mae_metric(y_batch, y_pred)\n",
    "    \n",
    "    \n",
    "    # get metrics and reset their states\n",
    "    test_loss_result = test_loss.result()\n",
    "    test_loss.reset_states()\n",
    "    \n",
    "    test_mae_result = test_mae_metric.result()\n",
    "    test_mae_metric.reset_states()\n",
    "    \n",
    "    \n",
    "    # print train logs\n",
    "    template = \"epoch {}: loss={:.3f}, val_loss={:3f}, mae={:.3f}, val_mae={:.3f}\"\n",
    "    print(template.format(epoch, train_loss_result, test_loss_result, train_mae_result, test_mae_result))\n",
    "    \n",
    "end = time.time()\n",
    "print(end - start, \"(sec)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0827 19:51:14.374773 11664 base_layer.py:1772] Layer dense_9 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: loss=10.335, val_loss=2.975882, mae=1.653, val_mae=0.912\n",
      "epoch 2: loss=2.382, val_loss=2.124567, mae=0.930, val_mae=0.846\n",
      "epoch 3: loss=1.757, val_loss=2.103338, mae=0.839, val_mae=0.866\n",
      "epoch 4: loss=1.866, val_loss=1.661618, mae=0.833, val_mae=0.767\n",
      "epoch 5: loss=1.204, val_loss=1.271695, mae=0.735, val_mae=0.679\n",
      "epoch 6: loss=1.003, val_loss=1.104320, mae=0.691, val_mae=0.612\n",
      "epoch 7: loss=0.977, val_loss=1.415838, mae=0.676, val_mae=0.727\n",
      "epoch 8: loss=1.225, val_loss=1.228926, mae=0.710, val_mae=0.664\n",
      "epoch 9: loss=0.884, val_loss=1.038066, mae=0.649, val_mae=0.617\n",
      "epoch 10: loss=0.879, val_loss=1.208016, mae=0.642, val_mae=0.735\n",
      "8.183550357818604 (sec)\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "loss_fn = keras.losses.MeanSquaredError()\n",
    "\n",
    "train_loss = keras.metrics.Mean()\n",
    "test_loss = keras.metrics.Mean()\n",
    "train_mae_metric = keras.metrics.MeanAbsoluteError()\n",
    "test_mae_metric = keras.metrics.MeanAbsoluteError()\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    #print(\"tracing:\", model, optimizer, x, y)\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y, y_pred)\n",
    "    grads = tape.gradient(loss, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    return y_pred, loss\n",
    "\n",
    "@tf.function\n",
    "def train_epoch(train_dataset):\n",
    "    for x_batch, y_batch in train_dataset:\n",
    "        \n",
    "        y_pred, loss = train_step(x_batch, y_batch)\n",
    "        \n",
    "        # accumulate metrics of one batch\n",
    "        train_loss(loss)\n",
    "        train_mae_metric(y_batch, y_pred)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def eval_step(x, y):\n",
    "    #print(\"tracing:\", model, x, y)\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y, y_pred)\n",
    "    return y_pred, loss\n",
    "    \n",
    "@tf.function\n",
    "def eval_epoch(test_dataset):\n",
    "    for x_batch, y_batch in test_dataset:\n",
    "        y_pred, loss = eval_step(x_batch, y_batch)\n",
    "        \n",
    "        # accumulate metrics\n",
    "        test_loss(loss)\n",
    "        test_mae_metric(y_batch, y_pred)\n",
    "\n",
    "    \n",
    "start = time.time()\n",
    "    \n",
    "for epoch in range(1, num_epochs+1):\n",
    "    \n",
    "    # train step \n",
    "    train_epoch(train_dataset)\n",
    "    \n",
    "    # get metrics of one epoch and reset their states\n",
    "    train_loss_result = train_loss.result()\n",
    "    train_loss.reset_states()\n",
    "    \n",
    "    train_mae_result = train_mae_metric.result()\n",
    "    train_mae_metric.reset_states()\n",
    "    \n",
    "    \n",
    "    # evaluation step\n",
    "    eval_epoch(test_dataset)\n",
    "    \n",
    "    \n",
    "    # get metrics and reset their states\n",
    "    test_loss_result = test_loss.result()\n",
    "    test_loss.reset_states()\n",
    "    \n",
    "    test_mae_result = test_mae_metric.result()\n",
    "    test_mae_metric.reset_states()\n",
    "    \n",
    "    \n",
    "    # print train logs\n",
    "    template = \"epoch {}: loss={:.3f}, val_loss={:3f}, mae={:.3f}, val_mae={:.3f}\"\n",
    "    print(template.format(epoch, train_loss_result, test_loss_result, train_mae_result, test_mae_result))\n",
    "    \n",
    "end = time.time()\n",
    "print(end - start, \"(sec)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0827 19:51:22.686779 11664 base_layer.py:1772] Layer dense_12 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: loss=8.643, val_loss=2.838111, mae=1.531, val_mae=0.925\n",
      "epoch 2: loss=2.292, val_loss=2.249154, mae=0.927, val_mae=0.783\n",
      "epoch 3: loss=1.619, val_loss=1.557780, mae=0.810, val_mae=0.692\n",
      "epoch 4: loss=1.399, val_loss=1.262913, mae=0.762, val_mae=0.674\n",
      "epoch 5: loss=1.201, val_loss=1.769752, mae=0.718, val_mae=0.916\n",
      "epoch 6: loss=1.505, val_loss=1.322963, mae=0.776, val_mae=0.681\n",
      "epoch 7: loss=1.117, val_loss=1.249477, mae=0.714, val_mae=0.670\n",
      "epoch 8: loss=1.120, val_loss=1.240016, mae=0.708, val_mae=0.684\n",
      "epoch 9: loss=0.906, val_loss=1.086592, mae=0.671, val_mae=0.693\n",
      "epoch 10: loss=1.065, val_loss=1.078038, mae=0.679, val_mae=0.623\n",
      "6.897313117980957 (sec)\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "loss_fn = keras.losses.MeanSquaredError()\n",
    "\n",
    "train_loss = keras.metrics.Mean()\n",
    "test_loss = keras.metrics.Mean()\n",
    "train_mae_metric = keras.metrics.MeanAbsoluteError()\n",
    "test_mae_metric = keras.metrics.MeanAbsoluteError()\n",
    "\n",
    "\n",
    "def train_step(x, y):\n",
    "    #print(\"tracing:\", model, optimizer, x, y)\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y, y_pred)\n",
    "    grads = tape.gradient(loss, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    return y_pred, loss\n",
    "\n",
    "@tf.function\n",
    "def train_epoch(train_dataset):\n",
    "    for x_batch, y_batch in train_dataset:\n",
    "        \n",
    "        y_pred, loss = train_step(x_batch, y_batch)\n",
    "        \n",
    "        # accumulate metrics of one batch\n",
    "        train_loss(loss)\n",
    "        train_mae_metric(y_batch, y_pred)\n",
    "\n",
    "\n",
    "\n",
    "def eval_step(x, y):\n",
    "    #print(\"tracing:\", model, x, y)\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y, y_pred)\n",
    "    return y_pred, loss\n",
    "    \n",
    "@tf.function\n",
    "def eval_epoch(test_dataset):\n",
    "    for x_batch, y_batch in test_dataset:\n",
    "        y_pred, loss = eval_step(x_batch, y_batch)\n",
    "        \n",
    "        # accumulate metrics\n",
    "        test_loss(loss)\n",
    "        test_mae_metric(y_batch, y_pred)\n",
    "\n",
    "    \n",
    "start = time.time()\n",
    "    \n",
    "for epoch in range(1, num_epochs+1):\n",
    "    \n",
    "    # train step \n",
    "    train_epoch(train_dataset)\n",
    "    \n",
    "    # get metrics of one epoch and reset their states\n",
    "    train_loss_result = train_loss.result()\n",
    "    train_loss.reset_states()\n",
    "    \n",
    "    train_mae_result = train_mae_metric.result()\n",
    "    train_mae_metric.reset_states()\n",
    "    \n",
    "    \n",
    "    # evaluation step\n",
    "    eval_epoch(test_dataset)\n",
    "    \n",
    "    \n",
    "    # get metrics and reset their states\n",
    "    test_loss_result = test_loss.result()\n",
    "    test_loss.reset_states()\n",
    "    \n",
    "    test_mae_result = test_mae_metric.result()\n",
    "    test_mae_metric.reset_states()\n",
    "    \n",
    "    \n",
    "    # print train logs\n",
    "    template = \"epoch {}: loss={:.3f}, val_loss={:3f}, mae={:.3f}, val_mae={:.3f}\"\n",
    "    print(template.format(epoch, train_loss_result, test_loss_result, train_mae_result, test_mae_result))\n",
    "    \n",
    "end = time.time()\n",
    "print(end - start, \"(sec)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tf2)",
   "language": "python",
   "name": "py37tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
