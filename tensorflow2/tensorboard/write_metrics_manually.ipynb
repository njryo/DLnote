{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-beta1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_toy_dataset(num_data):\n",
    "    X = np.random.randn(num_data, 3)\n",
    "    y = 3 * X[:, 0] - 2 * X[:, 1]**3 + 2 * X[:, 2]**2 + 0.5 * np.random.randn(num_data)\n",
    "    y = y[:, np.newaxis]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Dense(10, activation=\"relu\", input_dim=3),\n",
    "        keras.layers.Dense(10, activation=\"relu\"),\n",
    "        keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = 1000\n",
    "N_test = 200\n",
    "\n",
    "num_epochs = 25\n",
    "learning_rate = 0.01\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get toy datasets\n",
    "X_train, y_train = make_toy_dataset(N_train)\n",
    "X_test, y_test = make_toy_dataset(N_test)\n",
    "\n",
    "# make datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=512).batch(batch_size)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "test_dataset = test_dataset.batch(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "loss_fn = keras.losses.MeanSquaredError()\n",
    "\n",
    "train_loss = keras.metrics.Mean()\n",
    "test_loss = keras.metrics.Mean()\n",
    "train_mae_metric = keras.metrics.MeanAbsoluteError()\n",
    "test_mae_metric = keras.metrics.MeanAbsoluteError()\n",
    "\n",
    "\n",
    "logdir = os.path.join(\"logs\", \"manual_logging\")\n",
    "train_logdir = os.path.join(logdir, \"train\")\n",
    "test_logdir = os.path.join(logdir, \"test\")\n",
    "\n",
    "# create file writer\n",
    "train_summary_writer = tf.summary.create_file_writer(train_logdir)\n",
    "test_summary_writer = tf.summary.create_file_writer(test_logdir)\n",
    "\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    \n",
    "    # train step \n",
    "    for x_batch, y_batch in train_dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(x_batch)\n",
    "            loss = loss_fn(y_batch, y_pred)\n",
    "        grads = tape.gradient(loss, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        \n",
    "        # accumulate metrics of one batch\n",
    "        train_loss(loss)\n",
    "        train_mae_metric(y_batch, y_pred)\n",
    "    \n",
    "    \n",
    "    # save train metrics\n",
    "    with train_summary_writer.as_default():\n",
    "        tf.summary.scalar(\"loss\", train_loss.result(), step=epoch)\n",
    "        tf.summary.scalar(\"mae\", train_mae_metric.result(), step=epoch)\n",
    "    \n",
    "    train_loss.reset_states()\n",
    "    train_mae_metric.reset_states()\n",
    "    \n",
    "    \n",
    "    # evaluation step\n",
    "    for x_batch, y_batch in test_dataset:\n",
    "        y_pred = model(x_batch)\n",
    "        loss = loss_fn(y_batch, y_pred)\n",
    "        \n",
    "        # accumulate metrics\n",
    "        test_loss(loss)\n",
    "        test_mae_metric(y_batch, y_pred)\n",
    "    \n",
    "    \n",
    "    # save metrics of validation set\n",
    "    with test_summary_writer.as_default():\n",
    "        tf.summary.scalar(\"loss\", test_loss.result(), step=epoch)\n",
    "        tf.summary.scalar(\"mae\", test_mae_metric.result(), step=epoch)\n",
    "    \n",
    "    \n",
    "    test_loss.reset_states()\n",
    "    test_mae_metric.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tf2)",
   "language": "python",
   "name": "py37tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
